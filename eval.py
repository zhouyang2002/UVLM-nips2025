import json
import os
from openai import OpenAI
from tqdm import tqdm
import re
import ast

api_key = ""
client = OpenAI(api_key=api_key)

# Semantic Accuracy: 
# 1-2: Numerous errors; the description significantly differs from the standard answer with biological attribute errors (e.g., incorrect species classification).
# 3-4: Partially correct, but with significant errors (e.g., incorrect description of color, behavior, or environment parameters).
# 5-6: Mostly correct, but with minor discrepancies, such as imprecise descriptions of biological color, behavior, or ecological information.
# 7-8: Accurate description, closely matching the standard answer, with only minor variations in wording or structure.
# 9-10: Fully accurate, with the potential to add extra, valuable information such as ecological background or behavioral patterns.
# Detail Completeness:
# 1-2: Major details missing, with only a few key features provided (e.g., missing species name or main attributes).
# 3-4: Some key details missing, like an incomplete description of behavior, habitat, or color.
# 5-6: Contains the basic information but lacks some finer details, such as underwater environment or small biological features.
# 7-8: Detailed, matching the standard answer closely and covering most important details.
# 9-10: Extremely detailed, providing not just the standard answer’s content but also adding new, valuable information such as species ecological habits or environmental effects.
# Visual Perception Accuracy:
# 1-2: Misunderstands the image content entirely, such as incorrectly identifying the species or describing the wrong color, form, or background.
# 3-4: Partially correct, but with notable errors like a major mismatch in color, size, or behavior.
# 5-6: Mostly correct, but with some minor discrepancies in visual details such as color or posture.
# 7-8: Accurately matches the visual content, with only tiny errors.
# 9-10: Highly accurate, describing the image in great detail and potentially capturing visual information not included in the standard answer.
# Terminology Professionalism:
# 1-2: Uses incorrect or non-professional terminology, e.g., replacing scientific terms with casual language.
# 3-4: Some professional terminology, but with inaccuracies in certain fields (e.g., incorrect species classification).
# 5-6: Mostly professional, but missing high-precision terms or using slightly imprecise language.
# 7-8: Strong professionalism, with terminology close to that used in scientific literature.
# 9-10: Highly professional, with precise terminology fitting marine biology and ecology standards

prompt = """You are an assistant skilled at evaluating the quality of creative text.
Please act as an impartial judge and evaluate the performance of the InternVL model using a set of specific evaluation criteria. The task is to compare the output generated by InternVL with the provided standard answers.
You will need to assess the response on the following dimensions: Multiple‑Choice Accuracy (for the first four questions, each of which is strictly right or wrong), Semantic Accuracy, Detail Completeness, Visual Perception Accuracy, Environmental Description Accuracy, Species Behavior Matching.
We will provide you with the model’s response and a reference answer for your evaluation. As you begin your assessment, follow this process:
1. Evaluate the AI model's answers on different dimensions, pointing out its strengths or weaknesses in each dimension and assigning a score of 10 to 100 for each.
2. Finally, based on the assessments across dimensions, provide an overall score of 10 to 100 for the AI model's response.
3. Your scoring should be as stringent as possible and follow the scoring rules below:

Scoring rules:
Multiple‑Choice Accuracy:
Compute how many of the four multiple‑choice questions are correct.
0 correct → score 0
1 correct → score 25
2 correct → score 50
3 correct → score 75
4 correct → score 100

Semantic Accuracy:
10-20: Extremely rare cases only, completely incorrect identification or attributes entirely unrelated to the standard answer.
20-40: Clear inaccuracies exist but still partially recognizable, such as incorrect but similar species or noticeable attribute mistakes.
40-60: Generally correct with minor but noticeable inaccuracies in biological attributes, behaviors, or ecological details.
60-80: Accurate overall, minor wording or structural variations without substantial impact on correctness.
80-100: Fully accurate and may also include additional beneficial ecological or behavioral information.

Detail Completeness:
10-20: Only for substantial omission of critical information, significantly impacting overall understanding.
20-40: Basic information provided with several secondary details missing.
40-60: Adequately covers essential details but lacks certain finer or supplementary attributes.
60-80: Comprehensive description with most details thoroughly covered.
80-100: Exceptionally detailed, fully addressing the standard content and adding valuable supplementary information.

Visual Perception Accuracy:
10-20: Very rare, significant visual misinterpretation such as completely wrong species or setting.
20-40: Contains evident errors in major visual characteristics but broadly represents the correct visual scenario.
40-60: Mostly correct visually, minor mistakes in visual details such as slight discrepancies in posture or color.
60-80: Very accurate depiction with negligible minor errors allowed.
80-100: Outstandingly precise visual description, including potential additional insightful visual information beyond standard expectations

Environmental Description Accuracy:
10-20: Completely incorrect, e.g., confusing coral reef with sandy environments, or incorrect water type and light conditions.
20-40: Partially correct, but contains major errors, such as describing an environment feature that doesn't exist.
40-60: Mostly correct, but might miss some environmental details like water temperature, salinity, or light conditions.
60-80: Accurate environmental description, closely matching the standard answer.
80-100: Fully accurate, providing additional environmental information like ecosystem dynamics or potential environmental impacts.

Species Behavior Matching:
10-20: Describes behavior entirely incorrectly, such as confusing stationary behavior with movement or misinterpreting social behavior.
20-40: Some behavior is correctly described, but key behaviors like feeding or schooling are inaccurate.
40-60: Most behaviors are described correctly, but some aspects (e.g., subtle movements or interactions) are missing.
60-80: Accurate behavior description, with only slight variations.
80-100: Completely accurate, and may provide further analysis of species behavior, such as ecological significance or adaptability.

Please ensure that after your evaluation of each dimension, you also provide a detailed explanation for why you assigned that score. Then, provide the overall score based on the evaluations.

After your evaluations, format your response in the following dictionary format:
{
    "Multiple‑Choice Accuracy":["score"]
    "Semantic Accuracy": ["score", "reason"],
    "Detail Completeness": ["score", "reason"],
    "Visual Perception Accuracy": ["score", "reason"],
    "Environmental Description Accuracy": ["score", "reason"],
    "Species Behavior Matching": ["score", "reason"],
    "Overall Score": ["score", "reason"]
}
Thank you for your thorough evaluation.
"""
def extract_json(text):

    match = re.search(r'\{.*\}', text, re.DOTALL)
    if match:
        return match.group()
    return None

def load_json_files(folder_path):
    file_map = {}
    for file in os.listdir(folder_path):
        if file.endswith(".json"):
            with open(os.path.join(folder_path, file), 'r', encoding='utf-8') as f:
                file_map[file] = json.load(f)
    return file_map


def normalize_answers(data, target_key):
    normalized = []
    for item in data:
        if all(k in item for k in ("question", "answer", "generated")):
            normalized.append({
                "question": item["question"],
                "answer": item["answer"] if target_key == "answer" else item["generated"]
            })
    return normalized

def evaluate_model_response(reference_data, model_data):

    reference_list = normalize_answers(reference_data, "answer")
    model_list = normalize_answers(model_data, "generated")
    evaluation_prompt = f"""
    [model response]
    {json.dumps(model_list, indent=2, ensure_ascii=False)}

    [reference]
    {json.dumps(reference_list, indent=2, ensure_ascii=False)}

    """
    mes = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": evaluation_prompt},
    ]
    
    max_retries = 3
    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=mes,
                max_tokens=4096
            )
            # result = json.loads(response.choices[0].message.content)
            # return {k: tuple(v) if isinstance(v, list) else v for k,v in result.items()}
            response_text = response.choices[0].message.content
            json_str = extract_json(response_text)
            if json_str:
                result = json.loads(json_str)
                return result
            else:
                print("Not found")
        except json.JSONDecodeError:
            print(f"JSON fail，retry ({attempt+1}/{max_retries})")
            continue
        except Exception as e:
            print(f"Error: {str(e)}")
            return None
    return None


def evaluate_json_files(input_folder, reference_folder):
    input_files = load_json_files(input_folder)  
    reference_files = load_json_files(reference_folder)  
    
    results = []
    for filename in tqdm(input_files, desc="Process"):
        if filename not in reference_files:
            print(f"Warning：{filename} not found")
            continue
        
        result = evaluate_model_response(
            reference_files[filename]["content"],
            input_files[filename]["content"]
        )
        
        if result:
            results.append({
                "filename": filename,
                "result": result
            })
    return results

def main(input_dir, output_dir):
    input_files = load_json_files(input_dir)
    os.makedirs(output_dir, exist_ok=True)
    results = []

    for filename in tqdm(input_files, desc="Evaing"):
        data = input_files[filename]
        eval_result = evaluate_model_response(data, data)

        if eval_result:
            result_path = os.path.join(output_dir, f"eval_{filename}")
            with open(result_path, 'w', encoding='utf-8') as f:
                json.dump({
                    "filename": filename,
                    "result": eval_result
                }, f, ensure_ascii=False, indent=2)
            results.append(eval_result)

    
    if not results:
        print("⚠️ No Result")
        return

    summary = {
        "total_files": len(results),
        "average_scores": {
            dim: sum(res[dim][0] for res in results if dim in res) / len(results)
            for dim in results[0] if dim != "Overall Score"
        }
    }
    with open(os.path.join(output_dir, "summary.json"), 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)

    print("\n✅ Summary：")
    print(json.dumps(summary, indent=2, ensure_ascii=False))



if __name__ == "__main__":
    input_folder = ""       
    output_folder = ""  
    main(input_folder, output_folder)
